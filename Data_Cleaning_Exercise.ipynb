{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthomps/CIS-3902-Data-Mining/blob/main/Data_Cleaning_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkAJXjMuuMRW"
      },
      "source": [
        "# Data Cleaning in Python — A Hands-On Tutorial and Exercise prepared by Dr. Pamela Thompson\n",
        "\n",
        "**Course:** Intro to Data Mining  \n",
        "**Your Name:** Add your name\n",
        "**Dataset:** Titanic Passengers (loaded from a public URL, no file downloads needed!)  \n",
        "\n",
        "---\n",
        "\n",
        "## Why does data cleaning matter?\n",
        "\n",
        "Real-world data is *messy*. Before we can build any model or draw any conclusions, we need to:\n",
        "\n",
        "1. **Understand** the data (shape, types, distributions)\n",
        "2. **Find problems** (missing values, duplicates, outliers, wrong types)\n",
        "3. **Fix them** using appropriate strategies\n",
        "\n",
        "In practice, data scientists spend **60–80 %** of their time on data cleaning and preparation. This notebook walks through the most common cleaning steps using the famous Titanic dataset.\n",
        "\n",
        "> **Tip:** Run each cell one at a time and read the output before moving on. The goal is to understand *why* we do each step, not just *how*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8B5NMqguMRY"
      },
      "source": [
        "---\n",
        "## 1. Load the Data & Take a First Look\n",
        "\n",
        "We always start by loading the data and getting a quick overview. The key questions are:\n",
        "- How many rows and columns?\n",
        "- What do the first few rows look like?\n",
        "- What are the column data types?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI01qhjhuMRZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the full Titanic dataset from a public URL\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Quick peek: shape tells us (rows, columns)\n",
        "print(f'Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-53N2kkSuMRa"
      },
      "outputs": [],
      "source": [
        "# .head() shows the first 5 rows — always a good first step\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK42iO5puMRb"
      },
      "source": [
        "### What do the columns mean?\n",
        "\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| PassengerId | Unique ID for each passenger |\n",
        "| Survived | 0 = No, 1 = Yes (this is what we'd want to predict) |\n",
        "| Pclass | Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
        "| Name | Passenger name |\n",
        "| Sex | male / female |\n",
        "| Age | Age in years |\n",
        "| SibSp | # of siblings / spouses aboard |\n",
        "| Parch | # of parents / children aboard |\n",
        "| Ticket | Ticket number |\n",
        "| Fare | Passenger fare |\n",
        "| Cabin | Cabin number |\n",
        "| Embarked | Port of embarkation: C = Cherbourg, Q = Queenstown, S = Southampton |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLFrGOgruMRb"
      },
      "source": [
        "---\n",
        "## 2. Understand Data Types & Structure\n",
        "\n",
        "`.info()` is one of the most useful commands in pandas. It tells us:\n",
        "- The data type of each column (int, float, object)\n",
        "- How many **non-null** values each column has (helps spot missing data!)\n",
        "- Memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNflYBGOuMRb"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxKTeYd-uMRb"
      },
      "source": [
        "**What to notice above:**\n",
        "- `Age` has only 714 non-null values out of 891 → missing data!\n",
        "- `Cabin` has only 204 non-null values → a lot of missing data!\n",
        "- `Embarked` has 889 non-null values → just 2 missing\n",
        "- Columns like `Name`, `Sex`, `Ticket` are `object` (text) type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_q3f1qvuMRc"
      },
      "source": [
        "### What is the `object` data type?\n",
        "\n",
        "In pandas, **`object`** is the default type for columns that contain **text (strings)**. When you see `object` in the `.info()` output, it almost always means that column holds words/labels rather than numbers.\n",
        "\n",
        "**Why does this matter?** Most machine learning algorithms can **only work with numbers**. They can't do math on words like `\"male\"` or `\"Southampton\"`. So before we can use text columns in a model, we need to **convert them to numbers** — a process called **encoding**. We'll do this in Section 6.\n",
        "\n",
        "Here are the common pandas data types you'll see:\n",
        "\n",
        "| Dtype | What it means | Example values |\n",
        "|-------|--------------|----------------|\n",
        "| `int64` | Whole numbers | 1, 2, 3 |\n",
        "| `float64` | Decimal numbers | 7.25, 29.5 |\n",
        "| `object` | Text / strings | \"male\", \"S\", \"Braund, Mr. Owen\" |\n",
        "| `bool` | True/False | True, False |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q66wvd2uMRc"
      },
      "source": [
        "### Separate categorical vs. numerical columns\n",
        "\n",
        "This is important because different column types need different cleaning strategies.\n",
        "- **Numerical columns** → check for outliers, scale, impute with mean/median\n",
        "- **Categorical (`object`) columns** → check unique values, encode to numbers, impute with mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExAkIwb2uMRc"
      },
      "outputs": [],
      "source": [
        "# Identify categorical columns (dtype == 'object' means text/string)\n",
        "cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
        "\n",
        "# Identify numerical columns\n",
        "num_cols = [col for col in df.columns if df[col].dtype != 'object']\n",
        "\n",
        "print('Categorical columns:', cat_cols)\n",
        "print('Numerical columns: ', num_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLZvaAMDuMRc"
      },
      "outputs": [],
      "source": [
        "# For categorical columns, check how many unique values each has\n",
        "# This helps us decide which ones are useful for analysis\n",
        "df[cat_cols].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPtvlwa2uMRc"
      },
      "source": [
        "**Observation:** `Name` (891 unique) and `Ticket` (681 unique) have too many unique values to be useful as categorical features. We'll drop them later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYbOrQLEuMRd"
      },
      "source": [
        "---\n",
        "## 3. Descriptive Statistics\n",
        "\n",
        "`.describe()` gives us summary statistics for numerical columns. Look for:\n",
        "- Big differences between **mean** and **median (50%)** indicates possible skew or outliers\n",
        "- **min/max** values that seem unreasonable\n",
        "- The **count** row (does it match the total number of rows?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdDbQsTJuMRd"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_yqAZHuMRd"
      },
      "source": [
        "**Things to notice:**\n",
        "- `Age` count is 714, not 891 which confirms missing values\n",
        "- `Age` mean (29.7) and median/50% (28.0) are close and roughly symmetric\n",
        "- `Fare` mean (32.2) is much higher than median (14.5) and right-skewed (some very expensive tickets)\n",
        "- `Fare` min is 0.0 meaning some passengers paid nothing? Worth investigating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID6dVNLzuMRd"
      },
      "source": [
        "---\n",
        "## 4. Check for Duplicate Rows\n",
        "\n",
        "Duplicate rows can bias our analysis. We check for them and remove if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvr9ANUwuMRd"
      },
      "outputs": [],
      "source": [
        "# .duplicated() returns True for each row that is a duplicate of an earlier row\n",
        "num_duplicates = df.duplicated().sum()\n",
        "print(f'Number of duplicate rows: {num_duplicates}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT79akweuMRd"
      },
      "source": [
        "Good news — no duplicates in this dataset. If there were, we'd remove them with:\n",
        "```python\n",
        "df = df.drop_duplicates()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKqCC7uRuMRd"
      },
      "source": [
        "---\n",
        "## 5. Handle Missing Values\n",
        "\n",
        "This is often the biggest part of data cleaning. We have three main strategies:\n",
        "\n",
        "| Strategy | When to use |\n",
        "|----------|------------|\n",
        "| **Drop the column** | When most values are missing (e.g., > 50%) |\n",
        "| **Drop the rows** | When very few rows are affected |\n",
        "| **Fill (impute) the values** | When a moderate amount is missing and we want to keep the data |\n",
        "\n",
        "Let's first see exactly how much is missing in each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixvLzQmiuMRe"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of missing values for each column\n",
        "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
        "missing_pct = missing_pct.round(2)\n",
        "\n",
        "print('Percentage of missing values per column:')\n",
        "print(missing_pct[missing_pct > 0])  # Only show columns with missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KDJKFvauMRe"
      },
      "source": [
        "### Step 5a: Drop columns that won't be useful\n",
        "\n",
        "- `Cabin` is 77% missing — too much to fill reliably. **Drop it.**\n",
        "- `Name` and `Ticket` have too many unique values to be directly useful. **Drop them.**\n",
        "- `PassengerId` is just a row identifier, not a real feature. **Drop it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnC1Fir_uMRe"
      },
      "outputs": [],
      "source": [
        "# Drop columns that aren't useful for analysis\n",
        "cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
        "df_clean = df.drop(columns=cols_to_drop)\n",
        "\n",
        "print(f'Shape before: {df.shape}')\n",
        "print(f'Shape after:  {df_clean.shape}')\n",
        "df_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbrU0r6cuMRe"
      },
      "source": [
        "### Step 5b: Handle `Embarked` (only 2 missing values)\n",
        "\n",
        "Since only 2 rows out of 891 are missing, we can safely drop those rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVqg7UG5uMRe"
      },
      "outputs": [],
      "source": [
        "# Drop the 2 rows where Embarked is missing\n",
        "print(f'Rows before: {len(df_clean)}')\n",
        "df_clean = df_clean.dropna(subset=['Embarked'])\n",
        "print(f'Rows after:  {len(df_clean)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMMV1G4auMRf"
      },
      "source": [
        "### Step 5c: Handle `Age` (about 20% missing)\n",
        "\n",
        "20% is too much to just drop as we would lose a lot of data. Instead, we'll **impute** (fill in) the missing values.\n",
        "\n",
        "**Common imputation strategies:**\n",
        "- **Mean** — good when data is roughly normally distributed\n",
        "- **Median** — better when data is skewed or has outliers\n",
        "- **Mode** — used for categorical data\n",
        "\n",
        "Since Age's mean (29.7) and median (28.0) are close, either works here. We'll use the **median** since it's more robust to outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfgM14IBuMRf"
      },
      "outputs": [],
      "source": [
        "# Check the mean vs. median of Age before filling\n",
        "print(f'Age mean:   {df_clean[\"Age\"].mean():.1f}')\n",
        "print(f'Age median: {df_clean[\"Age\"].median():.1f}')\n",
        "\n",
        "# Fill missing Age values with the median\n",
        "df_clean['Age'] = df_clean['Age'].fillna(df_clean['Age'].median())\n",
        "\n",
        "# Verify: no more missing values\n",
        "print('\\nMissing values remaining:')\n",
        "print(df_clean.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkeMgN63uMRg"
      },
      "source": [
        "---\n",
        "## 6. Encode Categorical Variables\n",
        "\n",
        "Remember those `object` columns from Section 2? After dropping `Name`, `Ticket`, and `Cabin`, we still have two text columns left: **`Sex`** and **`Embarked`**. Machine learning algorithms need numbers, so we need to convert these text values into numerical form.\n",
        "\n",
        "There are two common encoding methods:\n",
        "\n",
        "| Method | How it works | Best for |\n",
        "|--------|-------------|----------|\n",
        "| **Label Encoding** | Assigns a number to each category (e.g., male=0, female=1) | Columns with only 2 categories, or categories with a natural order |\n",
        "| **One-Hot Encoding** | Creates a new column for each category with 0/1 values | Columns with 3+ categories that have no natural order |\n",
        "\n",
        "> **Why not just use label encoding for everything?** If we encode Embarked as C=0, Q=1, S=2, the model might think S > Q > C (i.e., that there's an order). One-hot encoding avoids this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NREq7qJkuMRg"
      },
      "source": [
        "### 6a: Label Encoding for `Sex` (2 categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY6PZArMuMRg"
      },
      "outputs": [],
      "source": [
        "# Sex has only 2 categories, so label encoding works well\n",
        "print('Before encoding:')\n",
        "print(df_clean['Sex'].value_counts())\n",
        "\n",
        "# Map text to numbers\n",
        "df_clean['Sex'] = df_clean['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "print('\\nAfter encoding:')\n",
        "print(df_clean['Sex'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgYEHNdWuMRh"
      },
      "source": [
        "### 6b: One-Hot Encoding for `Embarked` (3 categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4o1JiiKuMRh"
      },
      "outputs": [],
      "source": [
        "# Embarked has 3 categories (C, Q, S) with no natural order → use one-hot encoding\n",
        "print('Before encoding:')\n",
        "print(df_clean['Embarked'].value_counts())\n",
        "\n",
        "# pd.get_dummies() creates one new column per category\n",
        "# drop_first=True removes one column to avoid redundancy\n",
        "#   (if it's NOT Embarked_Q and NOT Embarked_S, it must be C)\n",
        "df_clean = pd.get_dummies(df_clean, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "print('\\nAfter encoding — new columns added:')\n",
        "df_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5nJdpxDuMRh"
      },
      "outputs": [],
      "source": [
        "# Check: no more 'object' columns!\n",
        "print('Data types after encoding:')\n",
        "print(df_clean.dtypes)\n",
        "print(f'\\nAll columns are now numerical: {all(df_clean.dtypes != \"object\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEjJGMYuMRi"
      },
      "source": [
        "Now every column is numerical and ready for machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5k17UkquMRi"
      },
      "source": [
        "---\n",
        "## 7. Detect and Handle Outliers\n",
        "\n",
        "Outliers are extreme values that are far from the rest of the data. They can distort statistics and model training.\n",
        "\n",
        "We'll use two common detection methods:\n",
        "1. **Visual inspection** with box plots\n",
        "2. **IQR (Interquartile Range) method** — a standard statistical approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8wxenXuuMRi"
      },
      "source": [
        "### 7a: Visualize with Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0wdfubkuMRj"
      },
      "outputs": [],
      "source": [
        "# Box plots for Age and Fare\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].boxplot(df_clean['Age'], vert=False)\n",
        "axes[0].set_xlabel('Age')\n",
        "axes[0].set_title('Box Plot of Age')\n",
        "\n",
        "axes[1].boxplot(df_clean['Fare'], vert=False)\n",
        "axes[1].set_xlabel('Fare')\n",
        "axes[1].set_title('Box Plot of Fare')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMA8pA2buMRj"
      },
      "source": [
        "The dots beyond the \"whiskers\" are potential outliers. Notice that `Fare` has some extreme values on the right side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn5iA_wTuMRj"
      },
      "source": [
        "### 7b: IQR Method for Outlier Detection\n",
        "\n",
        "The IQR method defines outliers as values that fall below `Q1 - 1.5 × IQR` or above `Q3 + 1.5 × IQR`.\n",
        "\n",
        "- **Q1** = 25th percentile\n",
        "- **Q3** = 75th percentile\n",
        "- **IQR** = Q3 − Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBF6fWxpuMRw"
      },
      "outputs": [],
      "source": [
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using the IQR method. Returns a boolean mask.\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
        "\n",
        "    print(f'{column}:')\n",
        "    print(f'  Q1 = {Q1:.2f}, Q3 = {Q3:.2f}, IQR = {IQR:.2f}')\n",
        "    print(f'  Lower bound = {lower_bound:.2f}, Upper bound = {upper_bound:.2f}')\n",
        "    print(f'  Number of outliers: {outliers.sum()} ({outliers.sum()/len(data)*100:.1f}%)')\n",
        "    return outliers\n",
        "\n",
        "age_outliers = detect_outliers_iqr(df_clean, 'Age')\n",
        "print()\n",
        "fare_outliers = detect_outliers_iqr(df_clean, 'Fare')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTuw_xywuMRw"
      },
      "source": [
        "### 7c: Remove Outliers from Age\n",
        "\n",
        "We'll remove Age outliers since they are a small percentage. We'll leave Fare outliers for now. Why? In the Titanic context, expensive first-class tickets are legitimate data points, not errors.\n",
        "\n",
        "> **Important:** Always think about the *domain context* before removing outliers. Not every extreme value is an error!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMHhDSC3uMRx"
      },
      "outputs": [],
      "source": [
        "# Remove Age outliers\n",
        "print(f'Rows before removing Age outliers: {len(df_clean)}')\n",
        "df_clean = df_clean[~age_outliers]\n",
        "print(f'Rows after:                        {len(df_clean)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3DjjRV7uMRx"
      },
      "source": [
        "---\n",
        "## 8. Feature Scaling (Normalization)\n",
        "\n",
        "Many machine learning algorithms work better when numerical features are on the **same scale**.\n",
        "\n",
        "| Method | Formula | Range | When to use |\n",
        "|--------|---------|-------|-------------|\n",
        "| **Min-Max Scaling** | (x - min) / (max - min) | [0, 1] | When you want bounded values |\n",
        "| **Standardization** | (x - mean) / std | ~[-3, 3] | When data is roughly normal |\n",
        "\n",
        "We'll demonstrate Min-Max scaling here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CVtyF-6uMRy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Get the numerical columns to scale\n",
        "# We exclude 'Survived' (target) and the one-hot columns (already 0/1)\n",
        "num_cols_to_scale = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "\n",
        "# Make a copy so we don't modify our cleaned data\n",
        "df_scaled = df_clean.copy()\n",
        "\n",
        "# Apply Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled[num_cols_to_scale] = scaler.fit_transform(df_scaled[num_cols_to_scale])\n",
        "\n",
        "print('Scaled data (first 5 rows):')\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdlChPcpuMRy"
      },
      "outputs": [],
      "source": [
        "# Verify: all scaled columns should now be between 0 and 1\n",
        "df_scaled[num_cols_to_scale].describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEKdB2y1uMRy"
      },
      "source": [
        "---\n",
        "## 9. Summary: Our Data Cleaning Pipeline\n",
        "\n",
        "Here's what we did, step by step:\n",
        "\n",
        "| Step | What we did | Why |\n",
        "|------|-------------|-----|\n",
        "| 1 | Loaded data, checked `.head()`, `.shape` | Understand the data |\n",
        "| 2 | Used `.info()` to check types and non-null counts | Find missing values and type issues |\n",
        "| 3 | Used `.describe()` for summary statistics | Spot anomalies, understand distributions |\n",
        "| 4 | Checked for duplicates with `.duplicated()` | Avoid double-counting |\n",
        "| 5a | Dropped columns: Cabin (77% missing), Name, Ticket, PassengerId | Remove unusable or irrelevant features |\n",
        "| 5b | Dropped 2 rows with missing Embarked | Very few missing — safe to drop |\n",
        "| 5c | Filled missing Age with the median | Preserve data; median is robust to outliers |\n",
        "| 6 | Encoded `Sex` (label) and `Embarked` (one-hot) | Convert text to numbers so algorithms can use them |\n",
        "| 7 | Detected outliers with IQR; removed Age outliers | Clean extreme values (with domain judgment) |\n",
        "| 8 | Applied Min-Max scaling to numerical features | Put features on same scale for modeling |"
      ]
    }
  ]
}
