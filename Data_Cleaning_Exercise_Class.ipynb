{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKuLdYtX0V3P"
      },
      "source": [
        "# Data Cleaning in Python — Exercise\n",
        "\n",
        "**Course:** Intro to Data Mining  \n",
        "**Dataset:** Heart Failure Prediction dataset (loaded from a public URL — no file downloads needed!)  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjIHH93e0V3n"
      },
      "source": [
        "# YOUR TURN: Practice Exercise\n",
        "\n",
        "Now apply the same data cleaning pipeline from the Titanic exercise to a **new dataset**: the **Heart Failure Prediction** dataset.\n",
        "\n",
        "This dataset combines 5 heart disease databases (918 patients total) and contains medical information about patients and whether or not they have heart disease. It has **text columns that need encoding**, potential outliers, and some tricky data quality issues — perfect for practicing everything we just learned.\n",
        "\n",
        "**Your tasks** (follow the same steps from above):\n",
        "\n",
        "1. Load the data and take a first look\n",
        "2. Understand the data types and structure\n",
        "3. Get descriptive statistics\n",
        "4. Check for duplicates\n",
        "5. Handle missing values (decide: drop column, drop row, or impute?)\n",
        "6. Encode any categorical (`object`) columns\n",
        "7. Detect and handle outliers\n",
        "8. Scale the numerical features\n",
        "\n",
        "The starter code below loads the dataset for you. The rest is up to you!\n",
        "\n",
        "> **Hints:** Pay close attention to `.describe()` — not all missing values show up as NaN! Also, this dataset was made by combining multiple hospital databases, so check for duplicates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbYy80jh0V3n"
      },
      "source": [
        "### About the Heart Failure Prediction Dataset\n",
        "\n",
        "| Column | Description | Type |\n",
        "|--------|-------------|------|\n",
        "| Age | Age in years | Numerical |\n",
        "| Sex | M = Male, F = Female | Categorical |\n",
        "| ChestPainType | TA = Typical Angina, ATA = Atypical Angina, NAP = Non-Anginal Pain, ASY = Asymptomatic | Categorical |\n",
        "| RestingBP | Resting blood pressure (mm Hg) | Numerical |\n",
        "| Cholesterol | Serum cholesterol (mg/dl) | Numerical |\n",
        "| FastingBS | Fasting blood sugar > 120 mg/dl (1 = true, 0 = false) | Numerical |\n",
        "| RestingECG | Normal, ST = ST-T wave abnormality, LVH = left ventricular hypertrophy | Categorical |\n",
        "| MaxHR | Maximum heart rate achieved | Numerical |\n",
        "| ExerciseAngina | Y = Yes, N = No | Categorical |\n",
        "| Oldpeak | ST depression induced by exercise | Numerical |\n",
        "| ST_Slope | Up = upsloping, Flat = flat, Down = downsloping | Categorical |\n",
        "| HeartDisease | 1 = heart disease, 0 = no heart disease (TARGET) | Numerical |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RedL8JE0V3n"
      },
      "source": [
        "### Step 1: Load the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NLX8nebE1U-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvyFlpZL0V3n"
      },
      "outputs": [],
      "source": [
        "# The Heart Failure Prediction dataset — loaded from a public GitHub URL\n",
        "# (Originally from Kaggle, combining 5 UCI heart disease databases)\n",
        "url = 'https://raw.githubusercontent.com/xpy-10/DataSet/main/heart.csv'\n",
        "\n",
        "heart = pd.read_csv(url)\n",
        "\n",
        "print(f'Dataset shape: {heart.shape[0]} rows x {heart.shape[1]} columns')\n",
        "heart.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60SStzxF0V3n"
      },
      "source": [
        "### Step 2: Check data types and structure\n",
        "\n",
        "Use `.info()` to see the data types and count non-null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiAObsNL0V3o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Hint: heart.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mm1Oz340V3o"
      },
      "source": [
        "### Step 3: Descriptive statistics\n",
        "\n",
        "Use `.describe()`. Look for anything surprising in the min, max, or mean values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV-ughqJ0V3o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Hint: heart.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsWQ25WV0V3o"
      },
      "source": [
        "### Step 4: Check for duplicates\n",
        "\n",
        "How many duplicate rows are there? If any, remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERlky-ul0V3o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Hint: heart.duplicated().sum()\n",
        "# To remove: heart = heart.drop_duplicates()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWdnMHSA0V3o"
      },
      "source": [
        "### Step 5: Handle missing values\n",
        "\n",
        "Check the percentage of missing values per column. Then decide what to do:\n",
        "- Drop columns that are mostly missing\n",
        "- Drop rows if very few are missing\n",
        "- Impute (fill) with mean or median if a moderate amount is missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVYe9GT30V3o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Step 1: Find missing value percentages\n",
        "# Hint: (heart.isnull().sum() / len(heart)) * 100\n",
        "\n",
        "# Step 2: Decide and apply your strategy for each column with missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_sUG7Jw0V3o"
      },
      "source": [
        "### Step 6: Encode categorical variables\n",
        "\n",
        "Check if any columns are `object` type. If so, decide whether to use label encoding or one-hot encoding.\n",
        "\n",
        "Hint: Look at `.info()` output — how many unique values does each `object` column have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oinYJla-0V3o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Hint: Check for object columns with:\n",
        "# heart.select_dtypes(include='object').columns\n",
        "#\n",
        "# For 2 categories: heart['col'] = heart['col'].map({'val1': 0, 'val2': 1})\n",
        "# For 3+ categories: heart = pd.get_dummies(heart, columns=['col'], drop_first=True)\n",
        "#\n",
        "# Think about which columns have 2 categories vs. 3+ categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3saJO7d0V3o"
      },
      "source": [
        "### Step 7: Detect and handle outliers\n",
        "\n",
        "Pick 2-3 numerical columns (e.g., `trestbps`, `chol`, `thalach`). Create box plots and use the IQR method.\n",
        "\n",
        "Remember: think about whether outliers are real data or errors before removing them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKOYmsIB0V3p"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Hint: You can reuse the detect_outliers_iqr function from above\n",
        "# Then create box plots with plt.boxplot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzVEV_4Z0V3p"
      },
      "source": [
        "### Step 8: Scale the numerical features\n",
        "\n",
        "Apply Min-Max scaling to the numerical columns (not the target column)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2xQMMaB0V3p"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Hint: Follow the same pattern from Section 8 of the tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3zDqR7Z0V3p"
      },
      "source": [
        "### Reflection Questions\n",
        "\n",
        "After completing the exercise, answer these questions (add a text cell or answer here):\n",
        "\n",
        "1. How did the Heart Disease dataset compare to the Titanic dataset in terms of data quality?\n",
        "2. Which missing value strategy did you use, and why?\n",
        "3. Did you find any outliers? Did you remove them? Why or why not?\n",
        "4. If you were building a model to predict heart disease, which columns do you think would be most important?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpvF5RKH0V3p"
      },
      "source": [
        "---\n",
        "*Great work! You now have hands-on experience with a complete data cleaning pipeline.* Dr. Thompson"
      ]
    }
  ]
}